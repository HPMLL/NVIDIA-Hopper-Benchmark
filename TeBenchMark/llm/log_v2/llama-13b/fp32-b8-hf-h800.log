Namespace(backend='hf', dtype='fp32', dataset='../ShareGPT_V3_unfiltered_cleaned_split.json', model='/workspace/dudayou/hf-models/hf-llama-2-13b/', tokenizer='/workspace/dudayou/hf-models/hf-llama-2-13b/', quantization=None, tensor_parallel_size=1, n=1, use_beam_search=False, num_prompts=504, seed=0, hf_max_batch_size=8, trust_remote_code=False, output_len=128, input_len=128)
tokenizering ...
finish tokenize !
loading model /workspace/dudayou/hf-models/hf-llama-2-13b/
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:22<00:44, 22.44s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:46<00:23, 23.54s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:02<00:00, 20.19s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:02<00:00, 20.98s/it]
/workspace/software/anaconda3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/workspace/software/anaconda3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/workspace/software/anaconda3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/workspace/software/anaconda3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
model created!! torch.float32 cuda:0
  0%|          | 0/504 [00:00<?, ?it/s]  2%|▏         | 8/504 [00:09<10:15,  1.24s/it]  3%|▎         | 16/504 [00:19<09:51,  1.21s/it]  5%|▍         | 24/504 [00:29<09:37,  1.20s/it]  6%|▋         | 32/504 [00:38<09:26,  1.20s/it]  8%|▊         | 40/504 [00:48<09:15,  1.20s/it] 10%|▉         | 48/504 [00:57<09:05,  1.20s/it] 11%|█         | 56/504 [01:07<08:55,  1.20s/it] 13%|█▎        | 64/504 [01:16<08:46,  1.20s/it] 14%|█▍        | 72/504 [01:26<08:36,  1.20s/it] 16%|█▌        | 80/504 [01:35<08:26,  1.20s/it] 17%|█▋        | 88/504 [01:45<08:17,  1.20s/it] 19%|█▉        | 96/504 [01:55<08:07,  1.19s/it] 21%|██        | 104/504 [02:04<07:57,  1.19s/it] 22%|██▏       | 112/504 [02:14<07:48,  1.19s/it] 24%|██▍       | 120/504 [02:23<07:38,  1.20s/it] 25%|██▌       | 128/504 [02:33<07:29,  1.20s/it] 27%|██▋       | 136/504 [02:42<07:19,  1.19s/it] 29%|██▊       | 144/504 [02:52<07:09,  1.19s/it] 30%|███       | 152/504 [03:01<07:00,  1.19s/it] 32%|███▏      | 160/504 [03:11<06:50,  1.19s/it] 33%|███▎      | 168/504 [03:21<06:41,  1.19s/it] 35%|███▍      | 176/504 [03:30<06:31,  1.19s/it] 37%|███▋      | 184/504 [03:40<06:22,  1.19s/it] 38%|███▊      | 192/504 [03:49<06:12,  1.19s/it] 40%|███▉      | 200/504 [03:59<06:03,  1.19s/it] 41%|████▏     | 208/504 [04:08<05:53,  1.19s/it] 43%|████▎     | 216/504 [04:18<05:44,  1.19s/it] 44%|████▍     | 224/504 [04:27<05:34,  1.19s/it] 46%|████▌     | 232/504 [04:37<05:24,  1.19s/it] 48%|████▊     | 240/504 [04:47<05:15,  1.19s/it] 49%|████▉     | 248/504 [04:56<05:05,  1.19s/it] 51%|█████     | 256/504 [05:06<04:56,  1.20s/it] 52%|█████▏    | 264/504 [05:15<04:46,  1.19s/it] 54%|█████▍    | 272/504 [05:25<04:37,  1.19s/it] 56%|█████▌    | 280/504 [05:34<04:27,  1.19s/it] 57%|█████▋    | 288/504 [05:44<04:18,  1.19s/it] 59%|█████▊    | 296/504 [05:53<04:08,  1.20s/it] 60%|██████    | 304/504 [06:03<03:59,  1.20s/it] 62%|██████▏   | 312/504 [06:13<03:49,  1.20s/it] 63%|██████▎   | 320/504 [06:22<03:39,  1.19s/it] 65%|██████▌   | 328/504 [06:32<03:30,  1.19s/it] 67%|██████▋   | 336/504 [06:41<03:20,  1.19s/it] 68%|██████▊   | 344/504 [06:51<03:11,  1.19s/it] 70%|██████▉   | 352/504 [07:00<03:01,  1.20s/it] 71%|███████▏  | 360/504 [07:10<02:52,  1.20s/it] 73%|███████▎  | 368/504 [07:20<02:42,  1.20s/it] 75%|███████▍  | 376/504 [07:29<02:33,  1.20s/it] 76%|███████▌  | 384/504 [07:39<02:23,  1.20s/it] 78%|███████▊  | 392/504 [07:48<02:13,  1.20s/it] 79%|███████▉  | 400/504 [07:58<02:04,  1.20s/it] 81%|████████  | 408/504 [08:07<01:54,  1.20s/it] 83%|████████▎ | 416/504 [08:17<01:45,  1.20s/it] 84%|████████▍ | 424/504 [08:27<01:35,  1.20s/it] 86%|████████▌ | 432/504 [08:36<01:26,  1.20s/it] 87%|████████▋ | 440/504 [08:46<01:16,  1.20s/it] 89%|████████▉ | 448/504 [08:55<01:07,  1.20s/it] 90%|█████████ | 456/504 [09:05<00:57,  1.20s/it] 92%|█████████▏| 464/504 [09:14<00:47,  1.20s/it] 94%|█████████▎| 472/504 [09:24<00:38,  1.20s/it] 95%|█████████▌| 480/504 [09:34<00:28,  1.20s/it] 97%|█████████▋| 488/504 [09:43<00:19,  1.20s/it] 98%|█████████▊| 496/504 [09:53<00:09,  1.20s/it]100%|██████████| 504/504 [10:02<00:00,  1.20s/it]100%|██████████| 504/504 [10:02<00:00,  1.20s/it]
Throughput: 0.84 requests/s, 214.06 tokens/s
