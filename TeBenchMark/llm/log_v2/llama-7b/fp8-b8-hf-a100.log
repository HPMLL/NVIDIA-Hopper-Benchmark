Namespace(backend='hf', dtype='fp8', dataset='../ShareGPT_V3_unfiltered_cleaned_split.json', model='/workspace/dudayou/hf-models/hf-llama-2-7b/', tokenizer='/workspace/dudayou/hf-models/hf-llama-2-7b/', quantization=None, tensor_parallel_size=1, n=1, use_beam_search=False, num_prompts=504, seed=0, hf_max_batch_size=8, trust_remote_code=False, output_len=128, input_len=128)
tokenizering ...
finish tokenize !
loading model /workspace/dudayou/hf-models/hf-llama-2-7b/
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:43<00:43, 43.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:57<00:00, 26.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:57<00:00, 28.83s/it]
/workspace/software/anaconda3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/workspace/software/anaconda3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/workspace/software/anaconda3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/workspace/software/anaconda3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
convert to te.linear
model created!! torch.bfloat16 cuda:0
  0%|          | 0/504 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/workspace/Hopper-Microbenchmark/TeBenchMark/llm/test.py", line 267, in <module>
    main(args)
  File "/workspace/Hopper-Microbenchmark/TeBenchMark/llm/test.py", line 195, in main
    elapsed_time = run_hf(requests, args.model, tokenizer, args.n,
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Hopper-Microbenchmark/TeBenchMark/llm/test.py", line 162, in run_hf
    with te.fp8_autocast(enabled= data_type == "fp8"):
  File "/workspace/software/anaconda3/lib/python3.11/contextlib.py", line 137, in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
  File "/workspace/software/anaconda3/lib/python3.11/site-packages/transformer_engine/pytorch/fp8.py", line 539, in fp8_autocast
    FP8GlobalStateManager.fp8_autocast_enter(
  File "/workspace/software/anaconda3/lib/python3.11/site-packages/transformer_engine/pytorch/fp8.py", line 394, in fp8_autocast_enter
    assert fp8_available, reason_for_no_fp8
AssertionError: Device compute capability 8.9 or higher required for FP8 execution.
  0%|          | 0/504 [00:00<?, ?it/s]
