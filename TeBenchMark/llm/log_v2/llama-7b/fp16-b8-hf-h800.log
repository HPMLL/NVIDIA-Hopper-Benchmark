Namespace(backend='hf', dtype='fp16', dataset='../ShareGPT_V3_unfiltered_cleaned_split.json', model='/workspace/dudayou/hf-models/hf-llama-2-7b/', tokenizer='/workspace/dudayou/hf-models/hf-llama-2-7b/', quantization=None, tensor_parallel_size=1, n=1, use_beam_search=False, num_prompts=504, seed=0, hf_max_batch_size=8, trust_remote_code=False, output_len=128, input_len=128)
tokenizering ...
finish tokenize !
loading model /workspace/dudayou/hf-models/hf-llama-2-7b/
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:18<00:18, 18.68s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 11.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 12.53s/it]
/workspace/software/anaconda3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/workspace/software/anaconda3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/workspace/software/anaconda3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/workspace/software/anaconda3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
model created!! torch.bfloat16 cuda:0
  0%|          | 0/504 [00:00<?, ?it/s]  2%|▏         | 8/504 [00:05<05:13,  1.58it/s]  3%|▎         | 16/504 [00:09<04:56,  1.65it/s]  5%|▍         | 24/504 [00:14<04:48,  1.66it/s]  6%|▋         | 32/504 [00:19<04:41,  1.67it/s]  8%|▊         | 40/504 [00:23<04:35,  1.68it/s] 10%|▉         | 48/504 [00:28<04:30,  1.69it/s] 11%|█         | 56/504 [00:33<04:25,  1.69it/s] 13%|█▎        | 64/504 [00:38<04:20,  1.69it/s] 14%|█▍        | 72/504 [00:42<04:15,  1.69it/s] 16%|█▌        | 80/504 [00:47<04:10,  1.69it/s] 17%|█▋        | 88/504 [00:52<04:05,  1.69it/s] 19%|█▉        | 96/504 [00:57<04:01,  1.69it/s] 21%|██        | 104/504 [01:01<03:56,  1.69it/s] 22%|██▏       | 112/504 [01:06<03:51,  1.69it/s] 24%|██▍       | 120/504 [01:11<03:46,  1.69it/s] 25%|██▌       | 128/504 [01:15<03:41,  1.69it/s] 27%|██▋       | 136/504 [01:20<03:37,  1.69it/s] 29%|██▊       | 144/504 [01:25<03:32,  1.69it/s] 30%|███       | 152/504 [01:30<03:27,  1.69it/s] 32%|███▏      | 160/504 [01:34<03:23,  1.69it/s] 33%|███▎      | 168/504 [01:39<03:18,  1.69it/s] 35%|███▍      | 176/504 [01:44<03:13,  1.69it/s] 37%|███▋      | 184/504 [01:48<03:09,  1.69it/s] 38%|███▊      | 192/504 [01:53<03:04,  1.69it/s] 40%|███▉      | 200/504 [01:58<02:59,  1.69it/s] 41%|████▏     | 208/504 [02:03<02:54,  1.69it/s] 43%|████▎     | 216/504 [02:07<02:49,  1.69it/s] 44%|████▍     | 224/504 [02:12<02:45,  1.69it/s] 46%|████▌     | 232/504 [02:17<02:40,  1.70it/s] 48%|████▊     | 240/504 [02:22<02:35,  1.69it/s] 49%|████▉     | 248/504 [02:26<02:31,  1.69it/s] 51%|█████     | 256/504 [02:31<02:26,  1.69it/s] 52%|█████▏    | 264/504 [02:36<02:21,  1.69it/s] 54%|█████▍    | 272/504 [02:40<02:17,  1.69it/s] 56%|█████▌    | 280/504 [02:45<02:12,  1.69it/s] 57%|█████▋    | 288/504 [02:50<02:07,  1.69it/s] 59%|█████▊    | 296/504 [02:55<02:02,  1.69it/s] 60%|██████    | 304/504 [02:59<01:58,  1.69it/s] 62%|██████▏   | 312/504 [03:04<01:53,  1.69it/s] 63%|██████▎   | 320/504 [03:09<01:49,  1.69it/s] 65%|██████▌   | 328/504 [03:14<01:44,  1.69it/s] 67%|██████▋   | 336/504 [03:18<01:39,  1.69it/s] 68%|██████▊   | 344/504 [03:23<01:34,  1.69it/s] 70%|██████▉   | 352/504 [03:28<01:29,  1.69it/s] 71%|███████▏  | 360/504 [03:33<01:25,  1.69it/s] 73%|███████▎  | 368/504 [03:37<01:20,  1.69it/s] 75%|███████▍  | 376/504 [03:42<01:15,  1.69it/s] 76%|███████▌  | 384/504 [03:47<01:10,  1.69it/s] 78%|███████▊  | 392/504 [03:51<01:06,  1.69it/s] 79%|███████▉  | 400/504 [03:56<01:01,  1.69it/s] 81%|████████  | 408/504 [04:01<00:56,  1.69it/s] 83%|████████▎ | 416/504 [04:06<00:52,  1.69it/s] 84%|████████▍ | 424/504 [04:10<00:47,  1.69it/s] 86%|████████▌ | 432/504 [04:15<00:42,  1.69it/s] 87%|████████▋ | 440/504 [04:20<00:37,  1.69it/s] 89%|████████▉ | 448/504 [04:24<00:33,  1.69it/s] 90%|█████████ | 456/504 [04:29<00:28,  1.69it/s] 92%|█████████▏| 464/504 [04:34<00:23,  1.69it/s] 94%|█████████▎| 472/504 [04:39<00:18,  1.69it/s] 95%|█████████▌| 480/504 [04:43<00:14,  1.69it/s] 97%|█████████▋| 488/504 [04:48<00:09,  1.69it/s] 98%|█████████▊| 496/504 [04:53<00:04,  1.69it/s]100%|██████████| 504/504 [04:58<00:00,  1.69it/s]100%|██████████| 504/504 [04:58<00:00,  1.69it/s]
Throughput: 1.69 requests/s, 432.91 tokens/s
